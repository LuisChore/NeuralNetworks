# -*- coding: utf-8 -*-
"""Tensorflow1_MNIST_Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fdzA4C_-a_MqH9WT3cONlucf7pEs2yu4

# Import Libraries & Headers
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
import tensorflow as tf
import numpy as np 
import matplotlib.pyplot as plt 
# %matplotlib inline

"""# 1. Import data set"""

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("input/data/",one_hot = True)

mnist.train.images.shape

image_example = np.reshape(mnist.train.images[24,:],(28,28))
plt.imshow(image_example,cmap = 'gray')

# function to print the image values 
def image_matrix(img):
  print('\n'.join([''.join(['{:4}'.format(int(round(item*255))) for item in row]) for row in img]))

image_matrix(image_example)

"""# 2. Set up the arquitecture

"""

'''
Tensorflow1 needs the architecture: how the neurons are connected 
'''

def neural_network_model(n_nodes_hl1=500,n_nodes_hl2=500,n_nodes_hl3=500,n_classes=10):
  
  # Set up the inputs & outputs
  '''
  Placeholders save space for the data
  It's generic because for every "run" the data is going to change
  It also validates the data type
  It validates the shape
    shape = [bash size for Stochastic GD, size of the input]
    bash_size = None, means that it's not fixed, I can change it
  Only the arguments that are defined are those that it  will validate

  '''
  x = tf.placeholder('float',[None,784])
  y = tf.placeholder('float')

  # Set up the variables (Weights & Bias)
  '''
  If something is declared as variable, the optimizer (algorithm) is going to 
  be able to change the value of those variables during the training, if
  something is declared as a constant, the algorithm won't try to modify it
  '''
  hidden_1_layer = {'weights':tf.Variable(tf.random_normal([784,n_nodes_hl1])),
                    'biases':tf.Variable(tf.random_normal([n_nodes_hl1]))}

  hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),
                    'biases':tf.Variable(tf.random_normal([n_nodes_hl2]))}

  hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),
                    'biases':tf.Variable(tf.random_normal([n_nodes_hl3]))}

  output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),
                    'biases':tf.Variable(tf.random_normal([n_classes]))}

  # Set up the architecture 
  '''
  Matrix operations to connect the network 
  A = W*X + B
  f(A), activation function: ReLU
  the tf.add, tf.matmul are optimized to run in CUDA
  '''
  l1 = tf.add(tf.matmul(x,hidden_1_layer['weights']),hidden_1_layer['biases'])
  l1 = tf.nn.relu(l1)

  l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'])
  l2 = tf.nn.relu(l2)

  l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'])
  l3 = tf.nn.relu(l3)

  output = tf.matmul(l3,output_layer['weights']) + output_layer['biases'] #not reccomended '+' 

  #Set cost function(error function) & Optimizer 
  '''
  Cost Function: (cross entropy with logits)
  Because there are (bash_size) of elements, we calculate the mean of the error
  Optimizer(Algorithm): AdamOptimizer, variant of GD
  '''

  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output,labels=y))
  optimizer = tf.train.AdamOptimizer().minimize(cost)

  # Return Control Dictionary
  '''
    Information that we want to use, all placeholders can be here
    
    Cost,Output, Any Layer Data: Tensor with data output
    Placeholders: Input values 
    Optimizer: Type 'NoOP' binary switch, execute it, or don't 
               Only if it is on changes the variables
  '''
  return dict(
      x = x,
      y = y,
      output = output,
      cost = cost,
      optimizer = optimizer
  )

# testing the model
'''
With this call, we are only declaring the architecture
Nothing have been executed, it's only the description
It returns only definitions: Cost,Optimizer, Output, x, y
'''

'''
HOW TENSORFLOW WORKS? 
Tensor Flow is made in Python (Interpreted Language)
But it works in Cuda(Compiled Language)
We still need to compiled it in order to work, we only have the definitions, the
pointers to those objects

'''
neural_network_model()

"""# 3. Training & Testing Function"""

'''
DNN: model
batch_size: number of data that is propagated

Tensorflow 1 doesn't make operations outside of a session
The session allows to compile the data and send it to CUDA 

Run Method:
This method runs one "step" of TensorFlow computation, by running the necessary 
graph fragment to execute every Operation and evaluate every Tensor in fetches,
substituting the values in feed_dict for the corresponding input values.

  ONE STEP:
     Feedforward the data, calculate the cost, optimize the variables

'''
def train_neural_network(DNN,epochs = 10,batch_size = 100):
  # all inside this is going to be compilated
  with tf.Session() as sess: 
    #initialize variables (weights,bias) async
    sess.run(tf.global_variables_initializer())

    for epoch in range(epochs):
      epoch_loss = 0
      for _ in range(int(mnist.train.num_examples/batch_size)):
        #data taken from the data set, to send them to the network
        epoch_x, epoch_y = mnist.train.next_batch(batch_size)
        '''
        Feed dictinary
          All the inputs to the DNN are going to be in feed_dict
        '''
        feed_dict={DNN["x"]:epoch_x,DNN["y"]:epoch_y}

        # all the input values in the first param are going to be returned 
        _,c,prediction,y = sess.run([DNN['optimizer'],DNN['cost'],
                                     DNN['output'],DNN['y']],
                                    feed_dict = feed_dict)
        epoch_loss += c
      print("Epoch",epoch, "completed out of",epochs,"loss:",epoch_loss)



    # TESTING WITH VALUES NOT SEEN #
    prediction,y = sess.run([DNN['output'],DNN['y']],feed_dict={DNN['x']:mnist.test.images,DNN['y']:mnist.test.labels})
    correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
    accuracy = tf.reduce_mean(tf.cast(correct,'float'))
    print('Accuracy:',accuracy.eval())

DNN =neural_network_model()
train_neural_network(DNN)